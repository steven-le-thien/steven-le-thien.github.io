<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dlt on TLE</title>
    <link>https://steven-le-thien.github.io/tags/dlt/</link>
    <description>Recent content in dlt on TLE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://steven-le-thien.github.io/tags/dlt/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Training invariances in deep nets and its consequences I: Set up and Problem description.</title>
      <link>https://steven-le-thien.github.io/p/training-invariances-in-deep-nets-and-its-consequences-i-set-up-and-problem-description./</link>
      <pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://steven-le-thien.github.io/p/training-invariances-in-deep-nets-and-its-consequences-i-set-up-and-problem-description./</guid>
      <description>Disclaimer: This is a &amp;lsquo;pilot&amp;rsquo; post where we take things slow and set up for subsequent discussions. Most of the information here is contained in any first course of Machine Learning and is written with minimal assumption on prior knowledge. Experienced readers may want skip to the next post in the series.
This post, and subsequent posts, aim to be self-content while carrying a narrative that builds up over time. As I try to balance between rigor and intuitiveness, I really appreciate any constructive feedback as to how I can improve my writings (one of the main reasons I started this blog!</description>
    </item>
    
  </channel>
</rss>
