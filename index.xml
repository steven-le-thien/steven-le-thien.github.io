<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>TLE</title>
        <link>https://steven-le-thien.github.io/</link>
        <description>Recent content on TLE</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 20 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://steven-le-thien.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Training invariances in deep nets and its consequences I: Set up and Problem description</title>
        <link>https://steven-le-thien.github.io/p/training-invariances-in-deep-nets-and-its-consequences-i-set-up-and-problem-description/</link>
        <pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate>
        
        <guid>https://steven-le-thien.github.io/p/training-invariances-in-deep-nets-and-its-consequences-i-set-up-and-problem-description/</guid>
        <description>&lt;!--image = &#34;matt-le-SJSpo9hQf7s-unsplash.jpg&#34;--&gt;
&lt;p&gt;Disclaimer: This is a &amp;lsquo;pilot&amp;rsquo; post where we take things slow and set up for subsequent discussions. Most of the information here is contained in any first course in Machine Learning and is written with minimal assumption on prior knowledge. Experienced readers may want skip to the next post in the series.&lt;/p&gt;
&lt;p&gt;This post, and subsequent posts, aim to be self-content while carrying a narrative that builds up over time. As I try to balance between rigor and intuitiveness, I really appreciate any constructive feedback as to how I can improve my writings (one of the main reasons I started this blog!) so that we all enjoy this process and learn a thing or two. I have annotated each section with &lt;code&gt;Basics&lt;/code&gt; if it is meant to provide basic information that readers who are familiar with machine learning setups can easily scheme through or straight up skip ahead.&lt;/p&gt;
&lt;h2 id=&#34;basics-set-up-of-binary-classification&#34;&gt;&lt;code&gt;Basics&lt;/code&gt; Set up of binary classification&lt;/h2&gt;
&lt;p&gt;We describe a simple binary classification problem that takes a training dataset $\mathcal{D} = \lbrace (x_i, y_i) \rbrace_{i \in [n]} $ with $n$ training examples. Each training example in the dataset is a pair: the first element is the input that lives in some space $\mathcal{X}$ and the second part is the label that is either $-1$ or $1$. For example, if the task is to build a model that can differentiate cats versus dogs given a picture of either kind, $\mathcal{X}$ can be the set of all pictures with fixed dimension and color channels; and the corresponding label $y$ to some training input  $x$ can be $-1$ if $x$ is a picture of a dog (this is training time, so we know the answer for each input  $x$ in the training dataset);  and  $1$ if it is a picture of a cat.&lt;/p&gt;
&lt;p&gt;That each training data point comes with a label makes  this  a &lt;em&gt;supervised learning&lt;/em&gt; problem and that the label can only take $2$ possible values makes this a &lt;em&gt;binary&lt;/em&gt; classification problem.&lt;/p&gt;
&lt;h2 id=&#34;basics-empirical-risk-minimization-erm&#34;&gt;&lt;code&gt;Basics&lt;/code&gt; Empirical risk minimization (ERM)&lt;/h2&gt;
&lt;p&gt;The above section describes the problem. Now we describe &lt;em&gt;one&lt;/em&gt; way to solve it by formulating a problem as a particular kind of optimization problem called ERM. Given a candidate solution model $f:\mathcal{X} \to \mathbb{R}$ and a single training input $x$, the label of  $x$ according to  $f$ is  $\text{sgn}(f(x)) \in \lbrace -1,1\rbrace$ where  $\text{sgn}(z)$ is  $1$ iff $z \geq 0$ and  $-1$ otherwise.&lt;/p&gt;
&lt;p&gt;We want to judge how good $f$ is doing on some training input $(x,y)$ using a loss function $\ell_f: \mathcal{X} \times \lbrace -1 , 1 \rbrace \to \mathbb{R}$ (some authors define the loss function differently but the main idea stays consistent). By convention, the smaller the value of $\ell_f$, the better  $f$ is performing on the classification task (thus explaining the terminology &amp;lsquo;loss&amp;rsquo; and &amp;lsquo;risk&amp;rsquo;). As an example, consider the exponential loss (which is not used all that often in practice due to numerical issues but is great to think about to derive some intuition):  $$\ell_f(x,y) := \exp(-yf(x)).$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Some sanity check: if $f$ predicts a large negative value while  $y = 1$, then we would want the loss to be a positive number (by convention) since $f$ is making a mistake with its classification. Indeed, if this is the case, then  $-yf(x)$ is a large positive number and so is $\ell_f(x,y)$. In the reverse case where $f$ makes a prediction that agrees with  $y$ in sign, it is also easy to verify that  $\ell_f(x,y)$ is small. In practice, people often use the logistic loss, which is closely related to the exponential loss presented here, since both, for example, are  minimized at $-\infty$ and are both monotonically decreasing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With some loss function, we have seen how we can judge $f$ on a single training input. When given a whole dataset of $n$ training inputs, the straightforward extension is called the empirical risk of $f$:
$$
{ \mathcal{R} }(f) := \frac{ 1 }{ n }  \sum_ { i = 1 } ^ n \ell_f(x_i,y_i).
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The name empirical risk comes from the fact that this is the expected loss over the empirical distribution given by the training data. The paradigm that tackles classification tasks by formulating an empirical risk and  then minimize it is called ERM.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;basics-optimization-and-parameterization&#34;&gt;&lt;code&gt;Basics&lt;/code&gt; Optimization and parameterization&lt;/h2&gt;
&lt;p&gt;If one can solve ERM efficiently, then one has a way to answer the classification task at hand, at least on the training examples. Unfortunately, without careful considerations and restraints, solving ERM can be very challenging. Recall that we have no restriction over possible candidate model $f$ so far, only that its domain is  $\mathcal{X}$ and its codomain is $\mathbb{R}$. Without any other restriction, the task of fining the minimizer of $\mathcal{R}$ is unthinkably hard. Therefore to make the optimization problem of minimizing $\mathcal{R}$ more tractable (and/or practical to run on a computer), a parameterization of $f$ can be used. For example, if   $\mathcal{X} = \R^d$ one constraints $f$ to be a linear function from  $\R^d \to \R$, then we know that all such linear functions take the form $f(z) = \langle z, a\rangle + b$ for some  $a \in \R^d$ and $b \in \R$ called parameters. Specifying a pair $(a,b)$ in the parameter space  $ \R^d \times \R = \R^{d+1}$ is equivalent to choosing a function $f$ from the space of all linear functions from  $\R^d \to \R$; but the former space is a lot nicer to work with, from an optimization perspective.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, we can use multivariate calculus tools to take the derivative of the risk with respect to $a$ or  $b$ and then invoke first order optimality condition (derivative equals to  $0$) to solve  for some $(a^*,b^*)$ ; but taking the derivative of the risk with respect to a function is vastly more and also needlessly complicated. In fact, we would almost always want the parameter space to be a subset of some Euclidean space in order to make use of the nice differential properties for optimization purposes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;neural-networks&#34;&gt;Neural networks&lt;/h2&gt;
&lt;p&gt;One particularly popular parameterization of function space is the use of neural networks. A fully-connected, feedforward neural network is a function defined as:
$$
f: \R ^ d \to \R :  z\mapsto \sigma_L (b_L + W_L \sigma_{L-1} (b_{L-1} + W_{L-1}\ldots \sigma_2(b_2 + W_2 \sigma_1(b_1 + (W_1z))) \ldots  )),
$$ where $L$ is the number of layers,  $\{\sigma_i: \R \to \R\}_{i \in [L]}$ are nonlinear functions ( called activation functions), $\lbrace \left( W_i \in \R^{ n _ { i+1 } } \times \R ^ { n _ i } , b_i \in \R^{ n _ { i + 1 } } \right) \rbrace _ {i \in [L]}$ are the parameters ( called  weight matrices and  biases, respectively) and $\lbrace n _ i \in \mathbb{N}\rbrace _ { i \in [L + 1] }$ are fixed architectural specifications (called the number of neurons in each layer) such that $n _ 1 = d$ and  $n _ { L+1 } = 1$ - corresponding to the input and output dimension of the neural network.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Another way to describe a neural network $f$ is by function composition. Let $p_i$ be affine functions defined as  $p_i(z) = W_iz + b_i$ for some parameters  $W_i$ and  $b_i$ that are matrices and vectors with appropriate dimensions, for all  $i \in [L]$, then we have:
$$
f = \sigma_L \circ p_L \circ \sigma_{L-1} \circ p_{L-1} \circ \ldots \circ \sigma_1 \circ p_1.
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The motivation for such a function class does not come anywhere close to satisfactorily explaining its miraculous performance in practice in recently years. Deeper theoretical understanding of deep neural networks (large $L$) is the main goal of deep learning theory.&lt;/p&gt;
&lt;h2 id=&#34;gradient-based-optimizers&#34;&gt;Gradient based optimizers&lt;/h2&gt;
&lt;p&gt;Parameterization of function space into simpler spaces such as Euclidean space paves way for generic optimization strategies called first order methods. The name come from the fact that these algorithms use only the gradient information of the objective that it is trying to optimize. A ubiquitous example is &lt;em&gt;gradient descent&lt;/em&gt; (GD): to find the minimizer of $\mathcal{R}(\theta)$ where $\theta \in \Theta \subseteq \R^k$ is a parameterization, assuming that the gradient of the risk with respect to $\theta$ is well-defined everywhere, the gradient descent algorithm initializes some $\theta_0$; and then iterates, say for a fixed, large number of times $T &amp;gt; 0$:
$$
\theta_{j + 1} = \theta_j - \eta_j\nabla \mathcal{R}(\theta_j), \qquad \forall j \in [T]
$$ where $\eta_j$ is the learning rate at step  $j$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Intuitively, gradient descent, at each steps, tries to pick the direction in which the objective function value decreases the fastest (given by the negative gradient direction) and then makes some progress based on the learning rate.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For theoretical analysis purposes, a lot of time, it is much more convenient to worth with continuous time instead of the discrete steps in gradient descent. The continuous time version of gradient descent is called &lt;em&gt;gradient flow&lt;/em&gt; (GF) and is described by the differential equation:
$$
\frac{ \textup{d} \theta(t) }{ \textup{d} t} = - \nabla \mathcal{R}(\theta)
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice that GF does not require choosing a step size.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One special property of GF - called the &lt;em&gt;descent property&lt;/em&gt; is that it never increases the value of the risk function (&lt;cite&gt;Ji and Telgarsky, 2019&lt;/cite&gt;):
$$
\frac{ \textup{d} \mathcal{R}(\theta(t))}{ \textup{d} t} = \left \langle \nabla \mathcal{R}(\theta), \frac{ \textup{d} \theta(t) }{ \textup{d} t} \right \rangle = - \| \nabla \mathcal{R}(\theta) \|^2 \leq 0
$$ where the first line is an application of the smooth chain rule and the second line is by definition of gradient flow.&lt;/p&gt;
&lt;p&gt;When the gradient of the risk is not guaranteed to exists everywhere but almost everywhere (for example, in the case of ReLU networks which we will define when needed), some notion of subdifferential $\partial$ is use and GF is a solution to the differential inclusion:
$$
\frac{ \textup{d} \theta(t) }{ \textup{d} t} \in - \partial \mathcal{R}(\theta).
$$ We will make such a notion clear when analyzing nondifferentiable networks.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, we have set up a &lt;em&gt;supervised learning&lt;/em&gt; problem for a binary classification task (think about differentiating dogs and cats) that involves training data (pictures of dogs/cats and corresponding labels). We have also described a popular paradigm used to tackle this problem: &lt;em&gt;empirical risk minimization&lt;/em&gt; via some loss function that judges the quality of some candidate function (based on its ability to differentiate dogs and cats pictures in the training data, without knowing the labels). Finally, we described certain generic &lt;em&gt;first order algorithms&lt;/em&gt; that &amp;lsquo;solve&amp;rsquo; the optimization problem at hand.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;p&gt;Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In International Conference on Learning Representations, 2019.&lt;/p&gt;

</description>
        </item>
        <item>
        <title>Training invariances in deep nets and its consequences II: Linear network</title>
        <link>https://steven-le-thien.github.io/p/training-invariances-in-deep-nets-and-its-consequences-ii-linear-network/</link>
        <pubDate>Sat, 20 Nov 2021 00:00:00 +0000</pubDate>
        
        <guid>https://steven-le-thien.github.io/p/training-invariances-in-deep-nets-and-its-consequences-ii-linear-network/</guid>
        <description>&lt;!--image = &#34;matt-le-SJSpo9hQf7s-unsplash.jpg&#34;--&gt;
&lt;p&gt;The training invariance that I will cover has appeared in Arora et al. (2018), Du et al. (2019) and Ji and Telgarsky (2019). I will mainly use the approach in Ji and Telgarsky (2019) since they are rather intuitive.&lt;/p&gt;
&lt;p&gt;Recall that from the last series, we have set up the binary classification problem with dataset $\lbrace (x_i \in \mathcal{X}, y_i \in \lbrace -1, 1 \rbrace ) \rbrace_{i \in [n]}$ of $n$ training examples. Fixing a nice loss function $\ell$, we set up the empirical risk minimization (ERM) problem  $$
\min_{w} \mathcal{R}(w) := \frac{1}{n} \sum _ { i = 1 } ^ n \ell( y_i f( x_i ; w))
$$ for some function class $\lbrace f(\cdot ; w) : \R^d \to \R \mid w \in R \rbrace$ parameterized by $w$ in some Euclidean vector space.&lt;/p&gt;
&lt;p&gt;To simplify the math, we use gradient flow (GF) to solve the optimization problem:
$$
\frac{ \textup{d} w(t) }{ \textup{d} t } = - \nabla_w \mathcal{ R }(w(t)).
$$ Unfamiliar readers can think of GF as the limit of the more famous gradient descent algorithm when we let step size goes to $0$ for intuition. With discrete steps, all results in this post still holds, albeit with extra slack terms.&lt;/p&gt;
&lt;h2 id=&#34;linear-neural-network-a-simple-but-not-that-simple-function-class&#34;&gt;Linear neural network: a simple (but not that simple) function class&lt;/h2&gt;
&lt;p&gt;The state of learning theory is such that a lot is known about linear models and classifiers but few nonlinear models come with provable guarantees. This is more or less true for deep learning theory where even advance analysis for nonlinear models depend heavily on the linear counterpart (say the recently-famous neural tangent kernel analysis). As such, much effort has been made to understand linear neural networks as we now define. A $L$-layer linear network with $n_i$ neurons in the  $i$-th layer for all  $i \in  [L+1]$ is parameterized by  $w = \lbrace W_k \in \R^{ n_{k+1} \times n_k } \rbrace_{k \in [L]}$ as
$$
f(x;w) = W_L W_{L-1} \ldots W_2 W_1 x.
$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Isn&amp;rsquo;t this just a linear classifier? Yes and no. As a function class, linear neural networks are indeed the same function class as linear classifiers: they contain exactly the same function. However, what we are interested in is the trajectory/behavior analysis of gradient based algorithms. As optimization objectives, ERM under linear nets is anything but similar to ERM under linear classifier. Indeed, a linear neural net, despite its name, is not a linear function in its parameters $w$ but a polynomial of degree  $L$.  Solving ERM for linear net under square loss, for example, is an instance of polynomial optimization - an NP-complete problem; whereas solving ERM for linear classifiers under squared loss is a version of the ordinary least square (OLS) problem (had we written our loss as $\ell(|f(x_i;w) - y_i|)$ then it would be exactly OLS).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;matrix-wise-training-invariance-a-three-line-proof-for-linear-neural-network&#34;&gt;Matrix-wise training invariance: a three-line proof for linear neural network&lt;/h2&gt;
&lt;p&gt;A mentor once told me: &amp;ldquo;Never underestimate the power of simply writing down the derivatives&amp;rdquo;, so we can start with that: for all $k \in  [ L ]$, for all time $t$ in the training process,
$$ \begin{equation}
\frac{ \textup{d} W _ k  } { \textup{d} t } = - \nabla_{ W_k } \mathcal{ R }( w ) = - \frac { 1 } { n } \sum_{ i = 1 } ^ n y _ i \ell &#39; ( y_i f ( x _ i ; w ) ) \nabla _ { W_k } f ( x _ i ; w )  = W_{ k+1 } ^ \top \ldots W _ L ^ \top A W _ 1 ^ \top \ldots W_{ k - 1 } ^ \top  \end{equation} $$ where $A :=- \nabla_{\left( W_L \ldots W_1 \right) } \mathcal{R}( f(w)) = -\frac{1}{n} \sum _ { i = 1 } ^ n y_ix_i \ell&#39; (y_i f(x_i))$. In the first step, we use definition of GF. In the second step, we use the smooth chain rule to differentiate through the loss function (and thus explicitly assumes that the loss is differentiable, although this also works for nondifferentiable losses). Finally, in the last step, we use the explicit formula of $f$ to differentiate  $W_k$ away.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here we used lots of tricks in multivariate calculus in order to be precise and formal. Unfamiliar readers can imagine all the weight matrices to be scalar and $f(x)  = w_L \ldots w_1 x$. Then differentiating $f$ with respect to  $w_k$ for some  $k$ will just remove  $w_k$ from the product and therefore we get the form of the last expression. When the weights are matrix, we have to be careful of the ordering of the weights since matrix multiplication is not commutative.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The main observation now is that the final right hand side expression contains all the weight matrices multiplied together, except for $W_k^\top$. One may wonder what happens if we right-multiply $W_k^\top$ back into the gradient expression. This is exactly the second line: for all time $t$ in the training process,
$$ \begin{equation} \frac{ \textup{d} W _ k } { \textup{d} t } W_k ^ \top  =  W_ { k+1 } ^ \top \ldots W _ L ^ \top A W _ 1 ^ \top \ldots W_{ k - 1 } ^ \top W_k ^\top  = W_ { k + 1 } ^ \top\frac{ \textup{d} W _ {k + 1} } { \textup{d} t } \end{equation} $$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that to get the second equality, we just write down the first derivation (eqn (1)) for $W_{k + 1}$ and notice the missing $W_{k+1}^\top$ term that we can left-multiply back into the gradient expression.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now I will admit that the third and last step requires more justifications than I am giving here if one is not familiar with multivariate calculus, but as usually, if this stumps you, imagine the weights matrices are just scalar and take the leap of faith. From equation (2), integrate both sides (via, for example, Fundamental Theorem of Calculus in each matrix entries) from $0$ to some time  $t &amp;gt; 0$  and rearrange to get
$$ \begin{equation} W_k(t) W_k^\top (t) - W_{k+1}^\top(t) W_{k+1}(t) = W_k(0) W_k^\top (0) -  W_{k+1}^\top(0) W_{k+1}(0) \end{equation} $$  and that is the first training invariance: the difference between self-adjoints of consecutive layers is set at initialization and remains invariant throughout training.&lt;/p&gt;
&lt;p&gt;Notice that we proved this invariance without any assumption on the initialization of GF: in fact the only extra assumption we made was that the loss is differentiable, which is not a problem for lots of popular loss functions like the logistic loss or the square loss.&lt;/p&gt;
&lt;p&gt;Why is this important? The remaining of the post will give some insights but here is a hint: the self-adjoint of a matrix controls its singular values, which in turn control the rank and deficiencies of the matrix.&lt;/p&gt;
&lt;h2 id=&#34;consequence-of-matrix-wise-invariance-in-linear-nets-low-rank-and-max-margin&#34;&gt;Consequence of matrix-wise invariance in linear nets: low rank and max-margin&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Coming soon&lt;/em&gt;&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
