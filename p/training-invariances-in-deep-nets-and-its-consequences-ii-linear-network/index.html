<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='In this second post of the series, we go through the one-line proof of a certain training invariance in linear neural networks and its consequences, in term of alignment, weight matrix rank, and margin. '><title>Training invariances in deep nets and its consequences II: Linear network</title>

<link rel='canonical' href='https://steven-le-thien.github.io/p/training-invariances-in-deep-nets-and-its-consequences-ii-linear-network/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='Training invariances in deep nets and its consequences II: Linear network'>
<meta property='og:description' content='In this second post of the series, we go through the one-line proof of a certain training invariance in linear neural networks and its consequences, in term of alignment, weight matrix rank, and margin. '>
<meta property='og:url' content='https://steven-le-thien.github.io/p/training-invariances-in-deep-nets-and-its-consequences-ii-linear-network/'>
<meta property='og:site_name' content='TLE'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='dlt' /><meta property='article:published_time' content='2021-11-20T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2021-11-20T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="Training invariances in deep nets and its consequences II: Linear network">
<meta name="twitter:description" content="In this second post of the series, we go through the one-line proof of a certain training invariance in linear neural networks and its consequences, in term of alignment, weight matrix rank, and margin. "><link rel="shortcut icon" href="/stitch.png">


<script async src="https://www.googletagmanager.com/gtag/js?id=G-H4DSHW5S7D"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-H4DSHW5S7D');
</script>



    </head>
    <body class="
    article-page has-toc
">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex 
    
        extended
    
">
    
        <div id="article-toolbar">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>Back</span>
            </a>
        </div>
    
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/deep-learning-theory/" style="background-color: #2a9d8f; color: white;">
                Deep Learning Theory
            </a>
        
    </header>
    

    <h2 class="article-title">
        <a href="/p/training-invariances-in-deep-nets-and-its-consequences-ii-linear-network/">Training invariances in deep nets and its consequences II: Linear network</a>
    </h2>

    
    <h3 class="article-subtitle">
        In this second post of the series, we go through the one-line proof of a certain training invariance in linear neural networks and its consequences, in term of alignment, weight matrix rank, and margin. 
    </h3>
    

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Nov 20, 2021</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    6 minute read
                </time>
            </div>
        
    </footer>
    
</div>
</header>

    <section class="article-content">
    <!--image = "matt-le-SJSpo9hQf7s-unsplash.jpg"-->
<p>The training invariance that I will cover has appeared in Arora et al. (2018), Du et al. (2019) and Ji and Telgarsky (2019). I will mainly use the approach in Ji and Telgarsky (2019) since they are rather intuitive.</p>
<p>Recall that from the last series, we have set up the binary classification problem with dataset $\lbrace (x_i \in \mathcal{X}, y_i \in \lbrace -1, 1 \rbrace ) \rbrace_{i \in [n]}$ of $n$ training examples. Fixing a nice loss function $\ell$, we set up the empirical risk minimization (ERM) problem  $$
\min_{w} \mathcal{R}(w) := \frac{1}{n} \sum _ { i = 1 } ^ n \ell( y_i f( x_i ; w))
$$ for some function class $\lbrace f(\cdot ; w) : \R^d \to \R \mid w \in R \rbrace$ parameterized by $w$ in some Euclidean vector space.</p>
<p>To simplify the math, we use gradient flow (GF) to solve the optimization problem:
$$
\frac{ \textup{d} w(t) }{ \textup{d} t } = - \nabla_w \mathcal{ R }(w(t)).
$$ Unfamiliar readers can think of GF as the limit of the more famous gradient descent algorithm when we let step size goes to $0$ for intuition. With discrete steps, all results in this post still holds, albeit with extra slack terms.</p>
<h2 id="linear-neural-network-a-simple-but-not-that-simple-function-class">Linear neural network: a simple (but not that simple) function class</h2>
<p>The state of learning theory is such that a lot is known about linear models and classifiers but few nonlinear models come with provable guarantees. This is more or less true for deep learning theory where even advance analysis for nonlinear models depend heavily on the linear counterpart (say the recently-famous neural tangent kernel analysis). As such, much effort has been made to understand linear neural networks as we now define. A $L$-layer linear network with $n_i$ neurons in the  $i$-th layer for all  $i \in  [L+1]$ is parameterized by  $w = \lbrace W_k \in \R^{ n_{k+1} \times n_k } \rbrace_{k \in [L]}$ as
$$
f(x;w) = W_L W_{L-1} \ldots W_2 W_1 x.
$$</p>
<blockquote>
<p>Isn&rsquo;t this just a linear classifier? Yes and no. As a function class, linear neural networks are indeed the same function class as linear classifiers: they contain exactly the same function. However, what we are interested in is the trajectory/behavior analysis of gradient based algorithms. As optimization objectives, ERM under linear nets is anything but similar to ERM under linear classifier. Indeed, a linear neural net, despite its name, is not a linear function in its parameters $w$ but a polynomial of degree  $L$.  Solving ERM for linear net under square loss, for example, is an instance of polynomial optimization - an NP-complete problem; whereas solving ERM for linear classifiers under squared loss is a version of the ordinary least square (OLS) problem (had we written our loss as $\ell(|f(x_i;w) - y_i|)$ then it would be exactly OLS).</p>
</blockquote>
<h2 id="matrix-wise-training-invariance-a-three-line-proof-for-linear-neural-network">Matrix-wise training invariance: a three-line proof for linear neural network</h2>
<p>A mentor once told me: &ldquo;Never underestimate the power of simply writing down the derivatives&rdquo;, so we can start with that: for all $k \in  [ L ]$, for all time $t$ in the training process,
$$ \begin{equation}
\frac{ \textup{d} W _ k  } { \textup{d} t } = - \nabla_{ W_k } \mathcal{ R }( w ) = - \frac { 1 } { n } \sum_{ i = 1 } ^ n y _ i \ell ' ( y_i f ( x _ i ; w ) ) \nabla _ { W_k } f ( x _ i ; w )  = W_{ k+1 } ^ \top \ldots W _ L ^ \top A W _ 1 ^ \top \ldots W_{ k - 1 } ^ \top  \end{equation} $$ where $A :=- \nabla_{\left( W_L \ldots W_1 \right) } \mathcal{R}( f(w)) = -\frac{1}{n} \sum _ { i = 1 } ^ n y_ix_i \ell' (y_i f(x_i))$. In the first step, we use definition of GF. In the second step, we use the smooth chain rule to differentiate through the loss function (and thus explicitly assumes that the loss is differentiable, although this also works for nondifferentiable losses). Finally, in the last step, we use the explicit formula of $f$ to differentiate  $W_k$ away.</p>
<blockquote>
<p>Here we used lots of tricks in multivariate calculus in order to be precise and formal. Unfamiliar readers can imagine all the weight matrices to be scalar and $f(x)  = w_L \ldots w_1 x$. Then differentiating $f$ with respect to  $w_k$ for some  $k$ will just remove  $w_k$ from the product and therefore we get the form of the last expression. When the weights are matrix, we have to be careful of the ordering of the weights since matrix multiplication is not commutative.</p>
</blockquote>
<p>The main observation now is that the final right hand side expression contains all the weight matrices multiplied together, except for $W_k^\top$. One may wonder what happens if we right-multiply $W_k^\top$ back into the gradient expression. This is exactly the second line: for all time $t$ in the training process,
$$ \begin{equation} \frac{ \textup{d} W _ k } { \textup{d} t } W_k ^ \top  =  W_ { k+1 } ^ \top \ldots W _ L ^ \top A W _ 1 ^ \top \ldots W_{ k - 1 } ^ \top W_k ^\top  = W_ { k + 1 } ^ \top\frac{ \textup{d} W _ {k + 1} } { \textup{d} t } \end{equation} $$</p>
<blockquote>
<p>Note that to get the second equality, we just write down the first derivation (eqn (1)) for $W_{k + 1}$ and notice the missing $W_{k+1}^\top$ term that we can left-multiply back into the gradient expression.</p>
</blockquote>
<p>Now I will admit that the third and last step requires more justifications than I am giving here if one is not familiar with multivariate calculus, but as usually, if this stumps you, imagine the weights matrices are just scalar and take the leap of faith. From equation (2), integrate both sides (via, for example, Fundamental Theorem of Calculus in each matrix entries) from $0$ to some time  $t &gt; 0$  and rearrange to get
$$ \begin{equation} W_k(t) W_k^\top (t) - W_{k+1}^\top(t) W_{k+1}(t) = W_k(0) W_k^\top (0) -  W_{k+1}^\top(0) W_{k+1}(0) \end{equation} $$  and that is the first training invariance: the difference between self-adjoints of consecutive layers is set at initialization and remains invariant throughout training.</p>
<p>Notice that we proved this invariance without any assumption on the initialization of GF: in fact the only extra assumption we made was that the loss is differentiable, which is not a problem for lots of popular loss functions like the logistic loss or the square loss.</p>
<p>Why is this important? The remaining of the post will give some insights but here is a hint: the self-adjoint of a matrix controls its singular values, which in turn control the rank and deficiencies of the matrix.</p>
<h2 id="consequence-of-matrix-wise-invariance-in-linear-nets-low-rank-and-max-margin">Consequence of matrix-wise invariance in linear nets: low rank and max-margin</h2>
<p><em>Coming soon</em></p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/dlt/">dlt</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer="true"
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    <aside class="related-contents--wrapper">
    
    
        <h2 class="section-title">Related contents</h2>
        <div class="related-contents">
            <div class="flex article-list--tile">
                
                    
<article class="">
    <a href="/p/training-invariances-in-deep-nets-and-its-consequences-i-set-up-and-problem-description/">
        
        

        <div class="article-details">
            <h2 class="article-title">Training invariances in deep nets and its consequences I: Set up and Problem description</h2>
        </div>
    </a>
</article>
                
            </div>
        </div>
    
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2024 TLE
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.5.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer="true"
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    
        <aside class="sidebar right-sidebar sticky">
            <section class="widget archives">
                <div class="widget-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



                </div>
                <h2 class="widget-title section-title">Table of contents</h2>
                
                <div class="widget--toc">
                    <nav id="TableOfContents">
  <ol>
    <li><a href="#linear-neural-network-a-simple-but-not-that-simple-function-class">Linear neural network: a simple (but not that simple) function class</a></li>
    <li><a href="#matrix-wise-training-invariance-a-three-line-proof-for-linear-neural-network">Matrix-wise training invariance: a three-line proof for linear neural network</a></li>
    <li><a href="#consequence-of-matrix-wise-invariance-in-linear-nets-low-rank-and-max-margin">Consequence of matrix-wise invariance in linear nets: low rank and max-margin</a></li>
  </ol>
</nav>
                </div>
            </section>
        </aside>
    

        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                defer="false"
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
