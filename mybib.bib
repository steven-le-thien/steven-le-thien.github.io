@article{chen2018characterizing,
  title={Characterizing glioblastoma heterogeneity via single-cell receptor quantification},
  author={Chen, Si and Le, Thien and Harley, Brendan AC and Imoukhuede, PI},
  journal={Frontiers in bioengineering and biotechnology},
  volume={6},
  pages={92},
  year={2018},
  publisher={Frontiers Media SA},
  url_Link={https://www.frontiersin.org/journals/bioengineering-and-biotechnology/articles/10.3389/fbioe.2018.00092/full},
  url_Paper={paper/chen2018characterizing.pdf}
}

@inproceedings{le2019using,
  title={Using inc within divide-and-conquer phylogeny estimation},
  author={Le, Thien and Sy, Aaron and Molloy, Erin K and Zhang, Qiuyi and Rao, Satish and Warnow, Tandy},
  booktitle={International Conference on Algorithms for Computational Biology},
  pages={167--178},
  year={2019},
  organization={Springer International Publishing Cham},
  url_Link={https://link.springer.com/chapter/10.1007/978-3-030-18174-1_12},
}

@article{le2020using,
  title={Using Constrained-INC for large-scale gene tree and species tree estimation},
  author={Le, Thien and Sy, Aaron and Molloy, Erin K and Zhang, Qiuyi and Rao, Satish and Warnow, Tandy},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  volume={18},
  number={1},
  pages={2--15},
  year={2020},
  publisher={IEEE},
  url_Link={https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093990},
  url_Paper={paper/le2020using.pdf}
}

@article{yu2020advancing,
  title={Advancing Divide-and-Conquer Phylogeny Estimation using Robinson-Foulds Supertrees},
  author={Yu, Xilin and Le, Thien and Christensen, Sarah A and Molloy, Erin K and Warnow, Tandy},
  journal={bioRxiv},
  pages={2020--05},
  year={2020},
  publisher={Cold Spring Harbor Laboratory},
  url_Link={https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.WABI.2020.15},
  url_Paper={paper/yu2020advancing.pdf}
}

@article{yu2021using,
  title={Using Robinson-Foulds supertrees in divide-and-conquer phylogeny estimation},
  author={Yu, Xilin and Le, Thien and Christensen, Sarah A and Molloy, Erin K and Warnow, Tandy},
  journal={Algorithms for Molecular Biology},
  volume={16},
  pages={1--18},
  year={2021},
  publisher={BioMed Central},
  url_Link={https://link.springer.com/article/10.1186/s13015-021-00189-2},
  url_Paper={paper/yu2021using.pdf}
}

@inproceedings{le2022training,
  title={Training invariances and the low-rank phenomenon: beyond linear networks},
  author={Le, Thien and Jegelka, Stefanie},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url_Link={https://arxiv.org/abs/2201.11968},
  url_Paper={paper/le2022training.pdf}
}

@article{le2023limits,
  title={Limits, approximation and size transferability for GNNs on sparse graphs via graphops},
  author={Le, Thien and Jegelka, Stefanie},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  url_Link={https://arxiv.org/abs/2306.04495},
  url_Paper={paper/le2023limits.pdf}
}

@inproceedings{le2024poincare,
  title={A Poincar{\'e} Inequality and Consistency Results for Signal Sampling on Large Graphs},
  author={Le, Thien and Ruiz, Luana and Jegelka, Stefanie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  abstract={Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit—the graphon. We prove a Poincare ́ inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related graphon signal sampling algorithm for large graphs, and demonstrate its good empirical performance on graph machine learning tasks.},
  keyword={signal processing, signal sampling, orthogonal basis, Poincar{\'e} inequality, Bernstein inequality, graphon, graph limit, spectral clustering, statistical consistency},
  url_Link={https://arxiv.org/abs/2311.10610},
  url_Paper={paper/le2024poincare.pdf}
}

@inproceedings{kiani2024hardness,
  title={On the hardness of learning under symmetries},
  author={Kiani, Bobak T. and Le, Thien and Lawrence, Hannah and Jegelka, Stefanie and Weber, Melanie},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  abstract={We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries (“equivariance”) into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension. Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard.},
  keyword={computational learning theory, statistical queries, correlational statistical queries, boolean functions, group symmetries, graph neural networks, NP hardness},
  url_Link={https://arxiv.org/abs/2401.01869},
  url_Paper={paper/kiani2024hardness.pdf}
}
