# jemdoc: menu{MENU}{index.html}, nofooter
= Thien Le

~~~
{}{img_left}{picture_pima.jpg}{picture_pima}{450px}{300px}{IMGLINKTARGET}
PhD researcher\n 
MIT Stata Center\n 
32 Vassar St\n
Cambridge, MA 02139\n
Email: thienle \[at\] mit (dot) edu\n 
Github: [https://github.com/steven-le-thien/ steven-le-thien]

~~~

== About me
I am a graduate student in [https://www.csail.mit.edu CSAIL/EECS] department at [https://www.mit.edu MIT] started in Fall 2019. I am fortunate to be advised by [https://people.csail.mit.edu/stefje/index.html Stefanie Jegelka]. I did my undergraduate in Mathematics and Computer Science  from 2016 to 2019 at [https://illinois.edu UIUC] where I was fortunate to work with [https://tandy.cs.illinois.edu Tandy Warnow] and her students on computational phylogenetics. Before that, I worked briefly in system biology with [https://sites.bioe.uw.edu/imoukhuede/ P.I. Imoukhuede] 


== Research
I am broadly interested in 
- Theory of (Geometric) Deep Learning 
- Learning under Invariances/Equivariances 
- Continuous Optimization
- Mathematical and Algorithmic Biology


My current research focuses on applying graph limit techniques to better understand generalization behaviors of deep learning model tailored to graph data (graph neural networks). In particular, I am interested in the following questions (with some references to my work on the topics): 

. Continuity: To what extent, and under what assumptions, can we guarantee classification/prediction consistency of the deep learning model when the input graphs are “structurally” similar?  
. Out-of-distribution size generalization: if a neural networks are trained to accurately predict a phenomenon on datasets of graphs of size $n$, can we expect it to behave decently on “structurally” similar datasets of size $N > n$? 
. Optimization: Can gradient-based learning algorithms on deep learning architectures learn graph datasets? In light of computational hardness of many graph problems, we do not expect to be able to train an efficient model for every task, so what exactly is learnt? 

At the heart of these questions are different ways to endow the space of graphs with topological/geometric structures. This is where graph limit tools, built on decades of work in graph theory - come into play. Graphons are powerful mathematical object that captures convergent sequence of dense graphs. Beyond graphons, I am very curious in answering these questions for sparse graphs, which are more prevalent in practice (our recent paper uses graphop to model sparse graph limit). There is still much work to be done in applying these tools to better understand deep learning models on graphs, with many untested ideas. 
